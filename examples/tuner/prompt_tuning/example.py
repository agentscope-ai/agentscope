"""Example of tuning a ReAct agent on GSM8K with Prompt Tuning."""

import os
from agentscope.agent._react_agent import ReActAgent
from agentscope.formatter._openai_formatter import OpenAIChatFormatter
from agentscope.message._message_base import Msg
from agentscope.model._dashscope_model import DashScopeChatModel
from agentscope.model._model_base import ChatModelBase
from agentscope.tuner._dataset import DatasetConfig
from agentscope.tuner._judge import JudgeOutput
from agentscope.tuner._workflow import WorkflowOutput
from agentscope.tuner.prompt_tune.config import PromptTuneConfig
from agentscope.tuner.prompt_tune.tune_prompt import tune_prompt


def workflow(sys_prompt: str):
    async def run_react_agent(
        task: dict,
        model: ChatModelBase,
        auxiliary_models: dict[str, ChatModelBase],
    ) -> WorkflowOutput:
        """A simple workflow function using the ReAct agent to solve tasks.

        Args:
            task (Dict): The task to be solved.
            model (TunerChatModel): The language model to use.
            auxiliary_models (Dict[str, TunerChatModel]):
                A dictionary of additional chat models available for
                LLM-as-a-Judge. Not used in this workflow.

        Returns:
            float: The reward obtained by solving the task.
        """
        assert (
            len(auxiliary_models) == 0
        ), "No auxiliary models are used in this workflow."
        
        from agentscope.tool import (
            Toolkit,
            execute_python_code,
        )
        
        toolkit=Toolkit()
        toolkit.register_tool_function(execute_python_code)
        agent = ReActAgent(
            name="react_agent",
            sys_prompt=sys_prompt,
            model=model,
            formatter=OpenAIChatFormatter(),
            toolkit=toolkit,
            print_hint_msg=False,
        )
        agent.set_console_output_enabled(False)

        response = await agent.reply(
            msg=Msg("user", task["question"], role="user"),
        )
        return WorkflowOutput(
            response=response,
        )

    return run_react_agent


async def gsm8k_judge(
    task: dict,
    response: Msg,
    auxiliary_models: dict[str, ChatModelBase],
) -> JudgeOutput:
    """A simple judge function to calculate reward based on agent's response.

    Args:
        task (Dict): The task information for the corresponding workflow.
        response (Msg): The response generated by the corresponding workflow.
        auxiliary_models (Dict[str, ChatModelBase]):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage. The keys are model names, and the values are the
            corresponding ChatModelBase instances.

    Returns:
        JudgeOutput: The reward value assigned by the judge function.
    """
    from trinity.common.rewards.math_reward import MathBoxedRewardFn

    assert (
        len(auxiliary_models) == 0
    ), "No auxiliary models are used in this workflow."

    reward_fn = MathBoxedRewardFn()
    # parse truth from gsm8k raw text
    truth = task["answer"]
    if isinstance(truth, str) and "####" in truth:
        truth = truth.split("####")[1].strip()
    else:
        truth = str(truth)
    # parse answer from response message
    result = response.get_text_content()
    reward_dict = reward_fn(
        response=result,
        truth=truth,
    )
    return JudgeOutput(
        reward=sum(reward_dict.values()),
        metrics=reward_dict,
    )


if __name__ == "__main__":

    init_prompt = (
        "You are an agent."
        "Please solve the math problem given to you with python code."
        "You should provife your output within \\boxed{{}}."
    )

    optimized_prompt = tune_prompt(
        workflow_func=workflow,
        init_system_prompt=init_prompt,
        judge_func=gsm8k_judge,
        train_dataset=DatasetConfig(
            path="train.parquet",
            name="",
            split="",
        ),
        eval_dataset=DatasetConfig(
            path="test.parquet",
            name="",
            split="",
        ),
        model=DashScopeChatModel(
            "qwen-flash", api_key=os.environ['DASHSCOPE_API_KEY'], max_tokens=512),
        config=PromptTuneConfig(
            lm_model_name="dashscope/qwen3-max",
            optimization_level='medium',
        )
    )

    print(f"Optimized prompt: {optimized_prompt}")


"""
2026-01-23 17:58:37,562 | INFO    | tune_prompt:tune_prompt:183 - optimized score: 96.88
2026-01-23 17:58:37,563 | INFO    | tune_prompt:tune_prompt:191 - improvement: 4.21%
2026-01-23 17:58:37,563 | INFO    | tune_prompt:tune_prompt:194 - ---------- Optimized Prompt ----------
2026-01-23 17:58:37,563 | INFO    | tune_prompt:tune_prompt:195 - You are a meticulous math tutor who solves elementary-to-middle-school-level word problems step by step. For each problem, first reason through the narrative to identify the key quantities and relationships. Then, write clear, executable Python code that computes the answer using only integer arithmetic. Finally, present your solution in the format \boxed{answer}, ensuring the answer is an integer and matches the logic of your explanation. Always double-check your reasoning and code before finalizing the boxed result.
2026-01-23 17:58:37,563 | INFO    | tune_prompt:tune_prompt:196 - --------------------------------------
Optimized prompt: You are a meticulous math tutor who solves elementary-to-middle-school-level word problems step by step. For each problem, first reason through the narrative to identify the key quantities and relationships. Then, write clear, executable Python code that computes the answer using only integer arithmetic. Finally, present your solution in the format \boxed{answer}, ensuring the answer is an integer and matches the logic of your explanation. Always double-check your reasoning and code before finalizing the boxed result.
"""