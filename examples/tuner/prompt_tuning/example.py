# -*- coding: utf-8 -*-
"""Example of tuning a ReAct agent on GSM8K with Prompt Tuning."""

import os
from agentscope.agent import ReActAgent
from agentscope.formatter import OpenAIChatFormatter
from agentscope.message import Msg
from agentscope.model import DashScopeChatModel
from agentscope.model import ChatModelBase
from agentscope.tuner import DatasetConfig
from agentscope.tuner import JudgeOutput
from agentscope.tuner import WorkflowOutput
from agentscope.tuner import PromptTuneConfig
from agentscope.tuner import tune_prompt


# Initialize the model for the workflow
model = DashScopeChatModel(
    "qwen-flash",
    api_key=os.environ.get("DASHSCOPE_API_KEY", ""),
    max_tokens=512,
)


async def workflow(
    task: dict,
    system_prompt: str,
) -> WorkflowOutput:
    """A workflow function using the ReAct agent to solve tasks.

    Args:
        task (dict): The task to be solved.
        system_prompt (str): The system prompt to use for the agent.

    Returns:
        WorkflowOutput: The workflow output containing the agent's response.
    """
    from agentscope.tool import (
        Toolkit,
        execute_python_code,
    )

    toolkit = Toolkit()
    toolkit.register_tool_function(execute_python_code)
    agent = ReActAgent(
        name="react_agent",
        sys_prompt=system_prompt,
        model=model,
        formatter=OpenAIChatFormatter(),
        toolkit=toolkit,
        print_hint_msg=False,
    )
    agent.set_console_output_enabled(False)

    response = await agent.reply(
        msg=Msg("user", task["question"], role="user"),
    )
    return WorkflowOutput(
        response=response,
    )


async def gsm8k_judge(
    task: dict,
    response: Msg,
) -> JudgeOutput:
    """A simple judge function to calculate reward based on agent's response.

    Args:
        task (Dict): The task information for the corresponding workflow.
        response (Msg): The response generated by the corresponding workflow.
        auxiliary_models (Dict[str, ChatModelBase]):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage. The keys are model names, and the values are the
            corresponding ChatModelBase instances.

    Returns:
        JudgeOutput: The reward value assigned by the judge function.
    """
    from trinity.common.rewards.math_reward import MathBoxedRewardFn

    reward_fn = MathBoxedRewardFn()
    # parse truth from gsm8k raw text
    truth = task["answer"]
    if isinstance(truth, str) and "####" in truth:
        truth = truth.split("####")[1].strip()
    else:
        truth = str(truth)
    # parse answer from response message
    result = response.get_text_content()
    reward_dict = reward_fn(
        response=result,
        truth=truth,
    )
    return JudgeOutput(
        reward=sum(reward_dict.values()),
        metrics=reward_dict,
    )


if __name__ == "__main__":
    init_prompt = (
        "You are an agent."
        "Please solve the math problem given to you with python code."
        "You should provife your output within \\boxed{{}}."
    )

    optimized_prompt = tune_prompt(
        workflow=workflow,
        init_system_prompt=init_prompt,
        judge_func=gsm8k_judge,
        train_dataset=DatasetConfig(
            path="train.parquet",
            name="",
            split="",
        ),
        eval_dataset=DatasetConfig(
            path="test.parquet",
            name="",
            split="",
        ),
        config=PromptTuneConfig(
            lm_model_name="dashscope/qwen3-max",
            optimization_level="medium",
        ),
    )

    print(f"Optimized prompt: {optimized_prompt}")
