# -*- coding: utf-8 -*-
"""Example of tuning a ReAct agent on GSM8K with Prompt Tuning."""

import os
from agentscope.agent._react_agent import ReActAgent
from agentscope.formatter._openai_formatter import OpenAIChatFormatter
from agentscope.message._message_base import Msg
from agentscope.model._dashscope_model import DashScopeChatModel
from agentscope.model._model_base import ChatModelBase
from agentscope.tuner._dataset import DatasetConfig
from agentscope.tuner._judge import JudgeOutput
from agentscope.tuner._workflow import WorkflowOutput, WorkflowType
from agentscope.tuner.prompt_tune.config import PromptTuneConfig
from agentscope.tuner.prompt_tune.tune_prompt import tune_prompt


def workflow(sys_prompt: str) -> WorkflowType:
    """
    Create a workflow function that uses the ReAct agent with a given
    system prompt.

    Args:
        sys_prompt (str): The system prompt to use for the ReAct agent.

    Returns:
        WorkflowType: A workflow function that takes task, model,
            and auxiliary_models as arguments and returns a WorkflowOutput.
    """

    async def run_react_agent(
        task: dict,
        model: ChatModelBase,
        auxiliary_models: dict[str, ChatModelBase],
    ) -> WorkflowOutput:
        """A simple workflow function using the ReAct agent to solve tasks.

        Args:
            task (Dict): The task to be solved.
            model (TunerChatModel): The language model to use.
            auxiliary_models (Dict[str, TunerChatModel]):
                A dictionary of additional chat models available for
                LLM-as-a-Judge. Not used in this workflow.

        Returns:
            float: The reward obtained by solving the task.
        """
        assert (
            len(auxiliary_models) == 0
        ), "No auxiliary models are used in this workflow."

        from agentscope.tool import (
            Toolkit,
            execute_python_code,
        )

        toolkit = Toolkit()
        toolkit.register_tool_function(execute_python_code)
        agent = ReActAgent(
            name="react_agent",
            sys_prompt=sys_prompt,
            model=model,
            formatter=OpenAIChatFormatter(),
            toolkit=toolkit,
            print_hint_msg=False,
        )
        agent.set_console_output_enabled(False)

        response = await agent.reply(
            msg=Msg("user", task["question"], role="user"),
        )
        return WorkflowOutput(
            response=response,
        )

    return run_react_agent


async def gsm8k_judge(
    task: dict,
    response: Msg,
    auxiliary_models: dict[str, ChatModelBase],
) -> JudgeOutput:
    """A simple judge function to calculate reward based on agent's response.

    Args:
        task (Dict): The task information for the corresponding workflow.
        response (Msg): The response generated by the corresponding workflow.
        auxiliary_models (Dict[str, ChatModelBase]):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage. The keys are model names, and the values are the
            corresponding ChatModelBase instances.

    Returns:
        JudgeOutput: The reward value assigned by the judge function.
    """
    from trinity.common.rewards.math_reward import MathBoxedRewardFn

    assert (
        len(auxiliary_models) == 0
    ), "No auxiliary models are used in this workflow."

    reward_fn = MathBoxedRewardFn()
    # parse truth from gsm8k raw text
    truth = task["answer"]
    if isinstance(truth, str) and "####" in truth:
        truth = truth.split("####")[1].strip()
    else:
        truth = str(truth)
    # parse answer from response message
    result = response.get_text_content()
    reward_dict = reward_fn(
        response=result,
        truth=truth,
    )
    return JudgeOutput(
        reward=sum(reward_dict.values()),
        metrics=reward_dict,
    )


if __name__ == "__main__":
    init_prompt = (
        "You are an agent."
        "Please solve the math problem given to you with python code."
        "You should provife your output within \\boxed{{}}."
    )

    optimized_prompt = tune_prompt(
        workflow_func=workflow,
        init_system_prompt=init_prompt,
        judge_func=gsm8k_judge,
        train_dataset=DatasetConfig(
            path="train.parquet",
            name="",
            split="",
        ),
        eval_dataset=DatasetConfig(
            path="test.parquet",
            name="",
            split="",
        ),
        model=DashScopeChatModel(
            "qwen-flash",
            api_key=os.environ["DASHSCOPE_API_KEY"],
            max_tokens=512,
        ),
        config=PromptTuneConfig(
            lm_model_name="dashscope/qwen3-max",
            optimization_level="medium",
        ),
    )

    print(f"Optimized prompt: {optimized_prompt}")
