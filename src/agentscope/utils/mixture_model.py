# -*- coding: utf-8 -*-
# pylint: disable=C0301
""" Utils for mixing model's answers in agentscope """

from typing import Union, List, Sequence

from agentscope.message import Msg
from agentscope.models import (
    ModelWrapperBase,
    ModelResponse,
    load_model_by_config_name,
)


# Referenced from the project [MoA](https://github.com/togethercomputer/MoA)
DEFAULT_AGGREGATOR_PROMPT = """You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.

Responses from models:"""  # noqa


class MoAModel:
    """
    The MoA model that take multiple models and aggregate their responses,
    leverages the collective strengths of multiple LLMs to enhance performance.
    Reference from the project [MoA](https://github.com/togethercomputer/MoA).
    """

    def __init__(
        self,
        main_model: Union[str, ModelWrapperBase],
        reference_models: List[Union[str, ModelWrapperBase]],
        aggregator_prompt: str = DEFAULT_AGGREGATOR_PROMPT,
    ) -> None:
        """
        Args:
            main_model (`Union[str, ModelWrapperBase]`):
                The main_model used for aggregation.
            reference_models (`List[Union[str, ModelWrapperBase]]`):
                The reference_models used for generating different responses.
                We encourage using different models to get better diversity.
                Emperically, responses generated by heterogeneous models
                contribute more than those produced by the same model.
            aggregator_prompt (`str`):
                The prompt used for aggregating responses.
                Using the prompt from paper MoA by default.
        """
        # init main_model
        if isinstance(main_model, str):
            self.main_model = load_model_by_config_name(main_model)
        elif isinstance(main_model, ModelWrapperBase):
            self.main_model = main_model
        else:
            raise ValueError(
                "main_model must be a string or a ModelWrapperBase instance",
            )

        # init reference_models
        self.reference_models: List[ModelWrapperBase] = []
        for ref_model in reference_models:
            if isinstance(ref_model, str):
                self.reference_models.append(
                    load_model_by_config_name(ref_model),
                )
            elif isinstance(ref_model, ModelWrapperBase):
                self.reference_models.append(ref_model)
            else:
                raise ValueError(
                    "reference_models must be a list of strings "
                    "or ModelWrapperBase instances",
                )

        self.aggregator_prompt = aggregator_prompt

    def format_and_call(
        self,
        *args: Union[Msg, Sequence[Msg]],
    ) -> ModelResponse:
        """
        Get model response from messages.
        Is equivalent to calling a model with:
            ```
            format_msg = model.format(messages)
            return model(format_msg)
            ```

        Args:
            *args (`Union[Msg, Sequence[Msg]]`):
                The messages to be sent to the model.
        """
        messages = []
        messages.append(
            Msg(role="system", content=self.aggregator_prompt, name="system"),
        )
        # TODO make this loop async or multi-threaded
        # to user our distributed module
        for ref_model in self.reference_models:
            format_msg = ref_model.format(*args)
            ref_model_res = ref_model(format_msg)
            # TODO findout the best way to present model names
            # with actual model names might induce bias
            messages.append(
                Msg(
                    role="user",
                    content=ref_model_res.text,
                    name=ref_model.model_name,
                ),
            )
        main_format_msg = self.main_model.format(messages)
        main_res = self.main_model(main_format_msg)
        return main_res
