"""Example of tuning a ReAct agent on GSM8K with Prompt Tuning."""

import os
from agentscope.agent._react_agent import ReActAgent
from agentscope.formatter._openai_formatter import OpenAIChatFormatter
from agentscope.message._message_base import Msg
from agentscope.model._dashscope_model import DashScopeChatModel
from agentscope.model._model_base import ChatModelBase
from agentscope.tuner._dataset import DatasetConfig
from agentscope.tuner._judge import JudgeOutput
from agentscope.tuner._workflow import WorkflowOutput
from agentscope.tuner.prompt_tune.tune_prompt import tune_prompt


def workflow(agent: ReActAgent):
    async def run_react_agent(
        task: dict,
        model: ChatModelBase,
        auxiliary_models: dict[str, ChatModelBase],
    ) -> WorkflowOutput:
        """A simple workflow function using the ReAct agent to solve tasks.

        Args:
            task (Dict): The task to be solved.
            model (TunerChatModel): The language model to use.
            auxiliary_models (Dict[str, TunerChatModel]):
                A dictionary of additional chat models available for
                LLM-as-a-Judge. Not used in this workflow.

        Returns:
            float: The reward obtained by solving the task.
        """
        assert (
            len(auxiliary_models) == 0
        ), "No auxiliary models are used in this workflow."
        # print("sysprompt: ",agent.sys_prompt)
        response = await agent.reply(
            msg=Msg("user", task["question"], role="user"),
        )
        return WorkflowOutput(
            response=response,
        )

    return run_react_agent


async def gsm8k_judge(
    task: dict,
    response: Msg,
    auxiliary_models: dict[str, ChatModelBase],
) -> JudgeOutput:
    """A simple judge function to calculate reward based on agent's response.

    Args:
        task (Dict): The task information for the corresponding workflow.
        response (Msg): The response generated by the corresponding workflow.
        auxiliary_models (Dict[str, ChatModelBase]):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage. The keys are model names, and the values are the
            corresponding ChatModelBase instances.

    Returns:
        JudgeOutput: The reward value assigned by the judge function.
    """
    from trinity.common.rewards.math_reward import MathBoxedRewardFn

    assert (
        len(auxiliary_models) == 0
    ), "No auxiliary models are used in this workflow."

    reward_fn = MathBoxedRewardFn()
    # parse truth from gsm8k raw text
    truth = task["answer"]
    if isinstance(truth, str) and "####" in truth:
        truth = truth.split("####")[1].strip()
    else:
        truth = str(truth)
    # parse answer from response message
    result = response.get_text_content()
    reward_dict = reward_fn(
        response=result,
        truth=truth,
    )
    return JudgeOutput(
        reward=sum(reward_dict.values()),
        metrics=reward_dict,
    )


if __name__ == "__main__":
    dataset = DatasetConfig(
        path="openai/gsm8k",
        name="main",
        split="train",
    )

    sys_prompt = (
        "You are an agent."
        "Please solve the math problem given to you."
        "You should provife your output within \\boxed{{}}."
    )
    agent = ReActAgent(
        name="react_agent",
        sys_prompt=sys_prompt,
        model=DashScopeChatModel(
            "qwen-turbo", api_key=os.environ['DASHSCOPE_API_KEY'], max_tokens=512),
        formatter=OpenAIChatFormatter(),
        print_hint_msg=False,
    )
    agent.set_console_output_enabled(False)

    agent=tune_prompt(
        workflow_func=workflow,
        init_agent=agent,
        judge_func=gsm8k_judge,
        train_dataset=dataset,
        model=DashScopeChatModel(
            "qwen-turbo", api_key=os.environ['DASHSCOPE_API_KEY'], max_tokens=512),
    )
