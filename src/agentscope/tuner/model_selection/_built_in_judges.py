# -*- coding: utf-8 -*-
"""Built-in judge functions for model selection."""

import time
from typing import Dict, Any, Dict as DictType
import asyncio
from agentscope.model import ChatModelBase
from agentscope.model._model_response import ChatResponse
from agentscope.model._model_usage import ChatUsage
from .._judge import JudgeType, JudgeOutput
from .._workflow import WorkflowOutput


async def avg_time_judge(
    task: Dict[str, Any],
    response: Any,
    auxiliary_models: DictType[str, ChatModelBase],
) -> JudgeOutput:
    """
    Built-in judge function to calculate average time consumption of a model.
    For this built-in function, smaller reward is better.

    Args:
        task (`Dict[str, Any]`):
            The task information for the corresponding workflow.
        response (`WorkflowOutput`):
           WorkflowOutput generated by the corresponding workflow.
        auxiliary_models (`Dict[str, ChatModelBase] | None`):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage.

    Returns:
        `JudgeOutput`:
            The time taken (since smaller time is better, and we want
            smaller reward to be better for this built-in function).
    """
    # Extract execution time from workflow output metrics
    time_taken = 0.0

    # Check if response contains workflow metrics (new approach)

    # Check if this is an enhanced response with workflow metrics
    if not isinstance(response, WorkflowOutput):
        raise ValueError("Response is not a WorkflowOutput")
    if "execution_time" not in response.metrics:
        raise ValueError("Missing 'execution' field in workflow_metrics")
    time_taken = response.metrics["execution_time"]
    reward = time_taken

    return JudgeOutput(
        reward=reward,
        metrics={"avg_time_seconds": time_taken, "raw_reward": time_taken},
    )


async def avg_token_consumption_judge(
    task: Dict[str, Any],
    response: Any,
    auxiliary_models: DictType[str, ChatModelBase],
) -> JudgeOutput:
    """
    Built-in judge function to calculate average token consumption of a model.
    For this built-in function, smaller reward is better.
    NOTE: The response parameter must include a 'usage' field containing token information.

    Args:
        task (`Dict[str, Any]`):
            The task information for the corresponding workflow.
        response (`Any`):
            The response field of the WorkflowOutput generated by the
            corresponding workflow. Must include a 'usage' field.
        auxiliary_models (`Dict[str, ChatModelBase] | None`):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage.

    Returns:
        `JudgeOutput`:
            The total token consumption (since lower usage is better,
            and we want smaller reward to be better for this built-in function).
    """
    # Check if response has a usage attribute/field
    if isinstance(response, WorkflowOutput):
        response = response.response
    if not hasattr(response, "usage") or response.usage is None:
        raise ValueError("Missing 'usage' field in response")

    usage = response.usage

    if "total_tokens" in usage and usage["total_tokens"] is not None:
        reward = float(usage["total_tokens"])
    elif "output_tokens" in usage and usage["output_tokens"] is not None:
        reward = float(usage["output_tokens"])
    else:
        raise ValueError(
            "Neither 'total_tokens' nor 'output_tokens' found in usage field"
        )

    # For token consumption judge, smaller token usage is better,
    # so we return the actual token count and the selection algorithm
    # will choose the smallest value
    return JudgeOutput(
        reward=reward,
        metrics={
            "token_consumed": reward,
        },
    )
