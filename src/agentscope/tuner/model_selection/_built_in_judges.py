# -*- coding: utf-8 -*-
"""Built-in judge functions for model selection."""

from typing import Dict, Any
from agentscope.model import ChatModelBase
from .._judge import JudgeOutput
from .._workflow import WorkflowOutput


async def avg_time_judge(
    _task: Dict[str, Any],
    response: WorkflowOutput,
    _auxiliary_models: Dict[str, ChatModelBase],
) -> JudgeOutput:
    """
    Built-in judge function to calculate average time consumption of a model.
    This function returns a negative reward (since smaller time usage is better,
    making it a bigger-is-better metric), and includes the original metric
    in the metrics field.

    Args:
        task (`Dict[str, Any]`):
            The task information for the corresponding workflow.
        response (`WorkflowOutput`):
           WorkflowOutput generated by the corresponding workflow.
        auxiliary_models (`Dict[str, ChatModelBase] | None`):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage.

    Returns:
        `JudgeOutput`:
            The negative time taken (making smaller time a bigger reward),
            and metrics containing the original time value.
    """
    # Extract execution time from workflow output metrics
    time_taken = 0.0

    # Check if response contains workflow metrics (new approach)

    # Check if this is an enhanced response with workflow metrics
    if not isinstance(response, WorkflowOutput):
        raise ValueError("Response is not a WorkflowOutput")
    if response.metrics is None:
        response.metrics = {}
    if "execution_time" not in response.metrics:
        raise ValueError("Missing 'execution' field in workflow_metrics")
    time_taken = response.metrics["execution_time"]

    # Return negative reward to make it bigger-is-better (smaller time = higher reward)
    reward = -time_taken

    return JudgeOutput(
        reward=reward,
        metrics={"avg_time_seconds": time_taken, "raw_reward": time_taken},
    )


async def avg_token_consumption_judge(
    _task: Dict[str, Any],
    response: WorkflowOutput,
    _auxiliary_models: Dict[str, ChatModelBase],
) -> JudgeOutput:
    """
    Built-in judge function to calculate average token consumption of a model.
    NOTE: The response parameter must include a 'usage' field containing token
    information.
    This function returns a negative reward (since smaller token usage is better,
    making it a bigger-is-better metric), and includes the original metric
    in the metrics field.

    Args:
        task (`Dict[str, Any]`):
            The task information for the corresponding workflow.
        response (`WorkflowOutput`):
            The response field of the WorkflowOutput generated by the
            corresponding workflow. Must include a 'usage' field.
        auxiliary_models (`Dict[str, ChatModelBase] | None`):
            A dictionary of additional chat models available for LLM-as-a-Judge
            usage.

    Returns:
        `JudgeOutput`:
            The negative token consumption (making smaller consumption a bigger reward),
            and metrics containing the original token consumption value.
    """
    # Check if response has a usage attribute/field
    if isinstance(response, WorkflowOutput):
        model_response = response.response
    if not hasattr(model_response, "usage") or model_response.usage is None:
        raise ValueError("Missing 'usage' field in response")

    usage = model_response.usage

    if "total_tokens" in usage and usage["total_tokens"] is not None:
        original_reward = float(usage["total_tokens"])
    elif "output_tokens" in usage and usage["output_tokens"] is not None:
        original_reward = float(usage["output_tokens"])
    else:
        raise ValueError(
            "Neither 'total_tokens' nor 'output_tokens' found in usage field",
        )

    # Return negative reward to make it bigger-is-better (smaller token usage = higher reward)
    reward = -original_reward

    return JudgeOutput(
        reward=reward,
        metrics={
            "token_consumed": original_reward,
        },
    )
