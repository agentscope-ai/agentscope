# -*- coding: utf-8 -*-
"""Built-in judge functions for model selection."""

from typing import Dict, Any
from .._judge import JudgeOutput
from .._workflow import WorkflowOutput


async def avg_time_judge(
    task: Dict[str, Any],
    response: WorkflowOutput,
) -> JudgeOutput:
    """
    Built-in judge function to calculate average time consumption of a model.
    This function returns a negative reward (since smaller time usage is better,
    making it a bigger-is-better metric), and includes the original metric
    in the metrics field.
    This function returns a negative reward (since smaller time usage is better,
    making it a bigger-is-better metric), and includes the original metric
    in the metrics field.

    Args:
        response (`WorkflowOutput`):
           WorkflowOutput generated by the corresponding workflow.

    Returns:
        `JudgeOutput`:
            The negative time taken (making smaller time a bigger reward),
            and metrics containing the original time value.
            The negative time taken (making smaller time a bigger reward),
            and metrics containing the original time value.
    """
    # Extract execution time from workflow output metrics
    time_taken = 0.0

    # Check if response contains workflow metrics (new approach)

    # Check if this is an enhanced response with workflow metrics
    if not isinstance(response, WorkflowOutput):
        raise ValueError("Response is not a WorkflowOutput")
    if response.metrics is None:
        response.metrics = {}
    if "execution_time" not in response.metrics:
        raise ValueError("Missing 'execution_time' field in workflow_metrics")
    time_taken = response.metrics["execution_time"]

    # Return negative reward to make it bigger-is-better (smaller time = higher reward)
    reward = -time_taken

    # Return negative reward to make it bigger-is-better (smaller time = higher reward)
    reward = -time_taken

    return JudgeOutput(
        reward=reward,
        metrics={"avg_time_seconds": time_taken},
        metrics={"avg_time_seconds": time_taken},
    )


async def avg_token_consumption_judge(
    task: Dict[str, Any],
    response: WorkflowOutput,
) -> JudgeOutput:
    """
    Built-in judge function to calculate average token consumption of a model.
    NOTE: The response parameter must include a 'metrics.usage' field containing token
    information or have a 'usage' attribute in the response object.
    This function returns a negative reward (since smaller token usage is better,
    making it a bigger-is-better metric), and includes the original metric
    in the metrics field.

    Args:
        response (`WorkflowOutput`):
            The response field of the WorkflowOutput generated by the
            corresponding workflow. Must include a 'metrics.usage' field or
            a 'usage' attribute in the response object.

    Returns:
        `JudgeOutput`:
            The negative token consumption (making smaller consumption a bigger reward),
            and metrics containing the original token consumption value.
            The negative token consumption (making smaller consumption a bigger reward),
            and metrics containing the original token consumption value.
    """
    original_reward = 0.0
    
    # First, try to get usage from response.metrics["usage"] (new approach)
    if isinstance(response, WorkflowOutput):
        if response.metrics and "usage" in response.metrics:
            usage = response.metrics["usage"]
            if isinstance(usage, dict):
                if "total_tokens" in usage and usage["total_tokens"] is not None:
                    original_reward = float(usage["total_tokens"])
                elif "output_tokens" in usage and usage["output_tokens"] is not None:
                    original_reward = float(usage["output_tokens"])
                else:
                    raise ValueError(
                        "Neither 'total_tokens' nor 'output_tokens' found in usage field"
                    )
            else:
                raise ValueError("Usage field in response.metrics is not a dictionary")
    else:
        raise ValueError("Response is not a WorkflowOutput")

    # Return negative reward to make it bigger-is-better (smaller token usage = higher reward)
    reward = -original_reward


    return JudgeOutput(
        reward=reward,
        metrics={
            "token_consumed": original_reward,
            "token_consumed": original_reward,
        },
    )
