{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# OpenJudge \u8bc4\u4f30\u5668\n\n[OpenJudge](https://github.com/agentscope-ai/OpenJudge) \u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30LLM/Agent\u5e94\u7528\u8d28\u91cf\u800c\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u901a\u8fc7\u5c06 OpenJudge \u96c6\u6210\u5230 AgentScope \u4e2d\uff0c\u60a8\u53ef\u4ee5\u5c06 AgentScope \u7684\u539f\u751f\u8bc4\u4f30\u80fd\u529b\u4ece\u57fa\u7840\u7684\u6267\u884c\u68c0\u67e5\u6269\u5c55\u5230\u6df1\u5ea6\u7684\u8bed\u4e49\u8d28\u91cf\u5206\u6790\u3002\n\n\u672c\u6307\u5357\u4e2d\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 OpenJudge \u7684\u8bc4\u4f30\u5668(Grader)\u4f5c\u4e3a AgentScope \u7684\u8bc4\u4f30\u6307\u6807(Metric)\u6765\u8bc4\u4f30\u60a8\u7684\u667a\u80fd\u4f53\u5e94\u7528\u3002\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\u5728\u8fd0\u884c\u672c\u6559\u7a0b\u4e4b\u524d\uff0c\u8bf7\u5b89\u88c5\u5fc5\u8981\u7684\u4f9d\u8d56\uff1a\n\n```bash\npip install agentscope py-openjudge</p></div>\n```\n## \u4e3a\u4ec0\u4e48\u9009\u62e9 OpenJudge\uff1f\n\n\u867d\u7136 AgentScope \u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8bc4\u4f30\u6846\u67b6\u7528\u4e8e\u5b9a\u4e49\u8bc4\u4f30\u903b\u8f91\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u7684\u8bed\u4e49\u7ea7\u6307\u6807\uff08\u5982\u201c\u5e7b\u89c9\u68c0\u6d4b\u201d\u6216\u201c\u56de\u590d\u76f8\u5173\u6027\u201d\uff09\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684 Prompt \u5de5\u7a0b\u548c\u6d41\u7a0b\u6784\u5efa\u5de5\u4f5c\u3002\n\n\u96c6\u6210 OpenJudge \u53ef\u4ee5\u4e3a AgentScope \u5e26\u6765\u4e86\u4e09\u4e2a\u7ef4\u5ea6\u7684\u80fd\u529b\u63d0\u5347\uff1a\n\n1.  **\u63d0\u5347\u8bc4\u4f30\u6df1\u5ea6**\uff1a\u4ece\u7b80\u5355\u7684\u6210\u529f/\u5931\u8d25\u68c0\u67e5\u5347\u7ea7\u4e3a\u591a\u7ef4\u5ea6\u7684\u8d28\u91cf\u8bc4\u4f30\uff08\u5982\u51c6\u786e\u6027\u3001\u5b89\u5168\u6027\u3001\u8bed\u6c14\u7b49\uff09\u3002\n2.  **\u5f00\u7bb1\u5373\u7528\u7684 Grader**\uff1a\u76f4\u63a5\u4f7f\u7528 50+ \u4e2a\u9884\u7f6e\u7684\u3001\u4e13\u5bb6\u7ea7\u9a8c\u8bc1\u8fc7\u7684 Grader\uff0c\u65e0\u9700\u624b\u52a8\u7f16\u5199\u8bc4\u4f30 Prompt\uff0c\u8be6\u60c5\u8bf7\u53c2\u9605 [OpenJudge\u5b98\u65b9\u6587\u6863](https://agentscope-ai.github.io/OpenJudge/built_in_graders/overview/)\u3002\n3.  **\u95ed\u73af\u8fed\u4ee3**\uff1a\u5c06 OpenJudge \u65e0\u7f1d\u5d4c\u5165 AgentScope \u7684 ``Benchmark`` \u4e2d\uff0c\u540c\u65f6\u83b7\u53d6\u91cf\u5316\u7684\u5206\u6570\u548c\u5b9a\u6027\u7684\u7406\u7531\u5206\u6790\u3002\n\n## \u5982\u4f55\u4f7f\u7528 OpenJudge \u8fdb\u884c\u8bc4\u4f30\n\n\u6211\u4eec\u5c06\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u7b54\uff08QA\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6f14\u793a\u5982\u4f55\u901a\u8fc7\u96c6\u6210 OpenJudge \u7684 Grader \u6765\u4f7f\u7528 AgentScope \u7684\u8bc4\u4f30\u6a21\u5757\u3002\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "QA_BENCHMARK_DATASET = [\n    {\n        \"id\": \"qa_task_1\",\n        \"question\": \"What are the health benefits of regular exercise?\",\n        \"reference_output\": \"Regular exercise improves cardiovascular health, strengthens muscles and bones, \"\n        \"helps maintain a healthy weight, and can improve mental health by reducing anxiety and depression.\",\n        \"ground_truth\": \"Answers should cover physical and mental health benefits\",\n        \"difficulty\": \"medium\",\n        \"category\": \"health\",\n    },\n    {\n        \"id\": \"qa_task_2\",\n        \"question\": \"Describe the main causes of climate change.\",\n        \"reference_output\": \"Climate change is primarily caused by increased concentrations of greenhouse gases \"\n        \"in the atmosphere due to human activities like burning fossil fuels, deforestation, and industrial processes.\",\n        \"ground_truth\": \"Answers should mention greenhouse gases and human activities\",\n        \"difficulty\": \"hard\",\n        \"category\": \"environment\",\n    },\n    {\n        \"id\": \"qa_task_3\",\n        \"question\": \"What is the significance of the Turing Test in AI?\",\n        \"reference_output\": \"The Turing Test, proposed by Alan Turing, is a measure of a machine's ability to exhibit\"\n        \" intelligent behavior equivalent to, or indistinguishable from, that of a human.\",\n        \"ground_truth\": \"Should mention Alan Turing, purpose of the test, and its implications for AI\",\n        \"difficulty\": \"hard\",\n        \"category\": \"technology\",\n    },\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AgentScope Metric vs. OpenJudge Grader\n\u4e3a\u4e86\u4f7f AgentScope \u517c\u5bb9 OpenJudge\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u9002\u914d\u5668\uff08Adapter\uff09\u6765\u5b8c\u6210\u4e24\u4e2a\u6846\u67b6\u95f4\u7684\u8f6c\u6362\u3002\n\u8fd9\u4e2a\u9002\u914d\u5668\u7ee7\u627f\u81ea AgentScope \u7684 ``MetricBase``\uff0c\u5e76\u5145\u5f53\u901a\u5f80 OpenJudge ``BaseGrader`` \u7684\u6865\u6881\u3002\n\n* **AgentScope Metric**: \u4e00\u4e2a\u901a\u7528\u7684\u8bc4\u4f30\u5355\u5143\uff0c\u63a5\u6536 ``SolutionOutput`` \u5e76\u8fd4\u56de ``MetricResult``\u3002\n* **OpenJudge Grader**: \u4e00\u4e2a\u7279\u5b9a\u7684\u8bc4\u4f30\u5355\u5143\uff08\u4f8b\u5982 ``RelevanceGrader``\uff09\uff0c\u9700\u8981\u7279\u5b9a\u7684\u8bed\u4e49\u8f93\u5165\uff08\u5982 ``query``, ``response``, ``context``\uff09\uff0c\u8fd4\u56de``GraderResult``\u3002\n\n\u8fd9\u4e2a\u201c\u9002\u914d\u5668\u201d\u5141\u8bb8\u60a8\u5c06 *\u4efb\u4f55* OpenJudge Grader \u65e0\u7f1d\u63d2\u5165\u5230\u60a8\u7684 AgentScope \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from openjudge.graders.base_grader import BaseGrader\nfrom openjudge.graders.schema import GraderScore, GraderError\nfrom openjudge.utils.mapping import parse_data_with_mapper\nfrom agentscope.evaluate import (\n    MetricBase,\n    MetricType,\n    MetricResult,\n    SolutionOutput,\n)\n\n\nclass OpenJudgeMetric(MetricBase):\n    def __init__(\n        self,\n        grader_cls: type[BaseGrader],\n        data: dict,\n        mapper: dict,\n        name: str | None = None,\n        description: str | None = None,\n        **grader_kwargs,\n    ):\n        \"\"\"Initializes the OpenJudgeMetric.\n\n        Args:\n            grader_cls (`type[BaseGrader]`):\n                The OpenJudge grader class to be wrapped.\n            data (`dict`):\n                The static data for the task.\n            mapper (`dict`):\n                The mapper to extract grader inputs from combined data.\n            name (`str | None`, optional):\n                The name of the metric. Defaults to the grader's name.\n            description (`str | None`, optional):\n                The description of the metric. Defaults to the grader's\n                description.\n            **grader_kwargs:\n                Additional keyword arguments for the grader initialization.\n        \"\"\"\n        self.grader = grader_cls(**grader_kwargs)\n\n        super().__init__(\n            name=name or self.grader.name,\n            metric_type=MetricType.NUMERICAL,\n            description=description or self.grader.description,\n        )\n\n        self.data = data\n        self.mapper = mapper\n\n    async def __call__(self, solution: SolutionOutput) -> MetricResult:\n        \"\"\"\u9488\u5bf9 Agent \u7684\u8f93\u51fa\u6267\u884c\u5c01\u88c5\u597d\u7684 OpenJudge Grader\u3002\"\"\"\n        if not solution.success:\n            return MetricResult(\n                name=self.name,\n                result=0.0,\n                message=\"Solution failed\",\n            )\n\n        try:\n            # 1. \u6784\u5efa\u4e0a\u4e0b\u6587\n            # \u5c06\u9759\u6001\u7684\u4efb\u52a1\u4e0a\u4e0b\u6587 (data) \u548c\u52a8\u6001\u7684 Agent \u8f93\u51fa (solution) \u7ec4\u5408\n            combined_data = {\n                \"data\": self.data,\n                \"solution\": {\n                    \"output\": solution.output,\n                    \"meta\": solution.meta,\n                    \"trajectory\": getattr(solution, \"trajectory\", []),\n                },\n            }\n\n            # 2. \u6570\u636e\u6620\u5c04\n            # \u4f7f\u7528 mapper \u4ece\u7ec4\u5408\u6570\u636e\u4e2d\u63d0\u53d6Grader\u9700\u8981\u7684 'query', 'response', 'context' \u7b49\u53c2\u6570\n            grader_inputs = parse_data_with_mapper(\n                combined_data,\n                self.mapper,\n            )\n\n            ## 3. \u6267\u884c\u8bc4\u4f30\n            result = await self.grader.aevaluate(**grader_inputs)\n\n            # 4. \u683c\u5f0f\u5316\u7ed3\u679c\n            if isinstance(result, GraderScore):\n                return MetricResult(\n                    name=self.name,\n                    result=result.score,\n                    # \u4fdd\u7559 OpenJudge \u63d0\u4f9b\u7684\u8be6\u7ec6\u7406\u7531\n                    message=result.reason or \"\",\n                )\n            elif isinstance(result, GraderError):\n                return MetricResult(\n                    name=self.name,\n                    result=0.0,\n                    message=f\"Error: {result.error}\",\n                )\n            else:\n                return MetricResult(\n                    name=self.name,\n                    result=0.0,\n                    message=\"Unknown result type\",\n                )\n\n        except Exception as e:\n            return MetricResult(\n                name=self.name,\n                result=0.0,\n                message=f\"Exception: {str(e)}\",\n            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u4ece OpenJudge \u5230 AgentScope\nOpenJudge \u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5185\u7f6e Grader \u96c6\u5408\u3002\u5728\u5f53\u524d\u5b9e\u4f8b\u4e2d\uff0c\u6211\u4eec\u9009\u62e9\u4e24\u4e2a\u9002\u5408\u95ee\u7b54\u4efb\u52a1\u7684\u5e38\u7528 Grader\uff1a\n\n* **RelevanceGrader (\u76f8\u5173\u6027)**\uff1a\u8bc4\u4f30 Agent \u7684\u56de\u7b54\u662f\u5426\u76f4\u63a5\u56de\u5e94\u4e86\u7528\u6237\u7684\u67e5\u8be2\uff08\u5ffd\u7565\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u3002\n* **CorrectnessGrader (\u6b63\u786e\u6027)**\uff1a\u6839\u636e\u63d0\u4f9b\u7684\u53c2\u8003\u7b54\u6848\uff08Ground Truth\uff09\u9a8c\u8bc1\u56de\u7b54\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\n\n.. tip::\n   OpenJudge \u63d0\u4f9b\u4e86 50+ \u79cd\u5185\u7f6e Grader\uff0c\u6db5\u76d6 **\u5e7b\u89c9\u68c0\u6d4b**\u3001**\u5b89\u5168\u6027**\u3001**\u4ee3\u7801\u8d28\u91cf** \u548c **JSON \u683c\u5f0f\u5316** \u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002\n   \u8bf7\u67e5\u9605 [OpenJudge \u5b98\u65b9\u6587\u6863](https://agentscope-ai.github.io/OpenJudge/built_in_graders/overview/) \u83b7\u53d6\u5b8c\u6574\u5217\u8868\u3002\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\u5728\u8fd0\u884c\u4ee5\u4e0b\u793a\u4f8b\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u60a8\u5df2\u8bbe\u7f6e ``DASHSCOPE_API_KEY`` \u73af\u5883\u53d8\u91cf\u3002</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom typing import Generator\nfrom openjudge.graders.common.relevance import RelevanceGrader\nfrom openjudge.graders.common.correctness import CorrectnessGrader\nfrom agentscope.evaluate import (\n    Task,\n    BenchmarkBase,\n)\n\n\nclass QABenchmark(BenchmarkBase):\n    def __init__(self):\n        super().__init__(\n            name=\"QA Quality Benchmark\",\n            description=\"Benchmark to evaluate QA systems using OpenJudge grader classes\",\n        )\n        self.dataset = self._load_data()\n\n    def _load_data(self):\n        tasks = []\n        # \u914d\u7f6e LLM Grader \u7684\u6a21\u578b\u53c2\u6570\n        # \u6ce8\u610f\uff1a\u5982\u679c\u4e0d\u4f7f\u7528\u73af\u5883\u53d8\u91cf\uff0c\u8bf7\u5728\u6b64\u5904\u8bbe\u7f6e \"api_key\"\n        model_config = {\n            \"model\": \"qwen3-32b\",\n            \"api_key\": os.environ.get(\"DASHSCOPE_API_KEY\"),\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        }\n\n        for data in QA_BENCHMARK_DATASET:\n            # \u5b9a\u4e49\u6620\u5c04\u5173\u7cfb\uff1a\u5de6\u4fa7\u662f OpenJudge \u7684\u952e\uff0c\u53f3\u4fa7\u662f AgentScope \u7684\u6570\u636e\u8def\u5f84\n            mapper = {\n                \"query\": \"data.input\",\n                \"response\": \"solution.output\",\n                \"context\": \"data.ground_truth\",\n                \"reference_response\": \"data.reference_output\",\n            }\n\n            # \u901a\u8fc7 Adapter \u5b9e\u4f8b\u5316 Metrics\n            metrics = [\n                OpenJudgeMetric(\n                    grader_cls=RelevanceGrader,\n                    data=data,\n                    mapper=mapper,\n                    name=\"Relevance\",\n                    model=model_config,\n                ),\n                OpenJudgeMetric(\n                    grader_cls=CorrectnessGrader,\n                    data=data,\n                    mapper=mapper,\n                    name=\"Correctness\",\n                    model=model_config,\n                ),\n            ]\n\n            # \u521b\u5efa Task\n            task = Task(\n                id=data[\"id\"],\n                input=data[\"question\"],\n                ground_truth=data[\"ground_truth\"],\n                metrics=metrics,\n            )\n\n            tasks.append(task)\n\n        return tasks\n\n    def __iter__(self) -> Generator[Task, None, None]:\n        yield from self.dataset\n\n    def __getitem__(self, index: int) -> Task:\n        return self.dataset[index]\n\n    def __len__(self) -> int:\n        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u8fd0\u884c\u8bc4\u4f30\n\u6700\u540e\uff0c\u4f7f\u7528 AgentScope \u7684 ``GeneralEvaluator`` \u5bf9\u4e00\u4e2a\u7b80\u5355\u7684QA Agent\u8fdb\u884c\u8bc4\u4f30\u6d4b\u8bd5\u3002\n\u6211\u4eec\u5c06\u6536\u96c6\u5230\u6765\u81ea OpenJudge Grader \u7684 **\u91cf\u5316\u5206\u6570 (Score)** \u548c **\u5b9a\u6027\u7406\u7531 (Reasoning)**\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nfrom typing import Callable\n\nfrom agentscope.agent import ReActAgent\nfrom agentscope.evaluate import GeneralEvaluator\nfrom agentscope.evaluate import FileEvaluatorStorage\nfrom agentscope.formatter import DashScopeChatFormatter\nfrom agentscope.message import Msg\nfrom agentscope.model import OpenAIChatModel\n\n\nasync def qa_agent(task: Task, pre_hook: Callable) -> SolutionOutput:\n    model = OpenAIChatModel(\n        model_name=\"qwen3-32b\",\n        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    )\n    agent = ReActAgent(\n        name=\"QAAgent\",\n        sys_prompt=\"You are an expert at answering questions. Provide clear, accurate, and comprehensive answers.\",\n        model=model,\n        formatter=DashScopeChatFormatter(),\n    )\n\n    # Generate response\n    msg_input = Msg(name=\"User\", content=task.input, role=\"user\")\n    response = await agent(msg_input)\n    response_text = response.content\n\n    return SolutionOutput(\n        success=True,\n        output=response_text,\n        trajectory=[\n            task.input,\n            response_text,\n        ],  # Store the interaction trajectory\n    )\n\n\nasync def main() -> None:\n    evaluator = GeneralEvaluator(\n        name=\"OpenJudge Integration Demo\",\n        benchmark=QABenchmark(),\n        # Repeat how many times\n        n_repeat=1,\n        storage=FileEvaluatorStorage(\n            save_dir=\"./results\",\n        ),\n        # How many workers to use\n        n_workers=1,\n    )\n\n    await evaluator.run(qa_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "~~~~~~~~~~\n\u6700\u540e\uff0c\u4f7f\u7528 AgentScope \u7684 ``GeneralEvaluator`` \u5bf9\u4e00\u4e2a\u7b80\u5355\u7684QA Agent\u8fdb\u884c\u8bc4\u4f30\u6d4b\u8bd5\u3002\n\u6211\u4eec\u5c06\u6536\u96c6\u5230\u6765\u81ea OpenJudge Grader \u7684 **\u91cf\u5316\u5206\u6570 (Score)** \u548c **\u5b9a\u6027\u7406\u7531 (Reasoning)**\u3002\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}