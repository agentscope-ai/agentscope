{
    "model_type": "openai_chat",
    "config_name": "vllm-llama-2",
    "model_name": "/mnt/bingchen/modelscope/Llama-2-7b-chat-ms",
    "api_key": "EMPTY",
    "client_args": {
        "base_url": "http://localhost:8000/v1/"
    },
    "generate_args": {
        "temperature": 0.5
    }
}