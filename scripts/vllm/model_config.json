{
    "model_type": "openai_chat",
    "config_name": "vllm-llama-2",
    "api_key": "EMPTY",
    "client_args": {
        "base_url": "http:localhost:8000/v1/"
    },
    "generate_args": {
        "temperature": 0.5
    }
}