{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Evaluation with OpenJudge\n\nThis guide introduces how to use [OpenJudge](https://github.com/agentscope-ai/OpenJudge) graders as AgentScope metrics to evaluate your multi-agent applications.\nOpenJudge is a comprehensive evaluation system designed to assess the quality of LLM applications. By integrating OpenJudge into AgentScope, you can extend AgentScope's native evaluation capabilities from basic execution checks to deep, semantic quality analysis.\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Install dependencies before running:\n\n```bash\npip install agentscope py-openjudge</p></div>\n```\n## Overview\nWhile AgentScope provides a robust `MetricBase` for defining evaluation logic, implementing complex, semantic-level metrics (like \"Hallucination Detection\" or \"Response Relevance\") often requires\nsignificant effort in prompt engineering and pipeline construction.\n\nIntegrating OpenJudge brings three dimensions of capability extension to AgentScope:\n\n1.  **Enhance Evaluation Depth:**: Move beyond simple success/failure checks to multi-dimensional assessments (Accuracy, Safety, Tone, etc.).\n2.  **Leverage Verified Graders**: Instantly access 50+ pre-built, expert-level graders without writing custom evaluation prompts, see the [OpenJudge documentation](https://agentscope-ai.github.io/OpenJudge/built_in_graders/overview/) for details.\n3.  **Closed-loop Iteration**: Seamlessly embed OpenJudge into AgentScope's `Benchmark`, obtaining quantitative scores and qualitative reasoning.\n\n\n## How to Evaluate with OpenJudge\n\nWe are going to build a simple QA benchmark to demonstrate how to use the AgentScope evaluation module by integrating OpenJudge's graders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "QA_BENCHMARK_DATASET = [\n    {\n        \"id\": \"qa_task_1\",\n        \"question\": \"What are the health benefits of regular exercise?\",\n        \"reference_output\": \"Regular exercise improves cardiovascular health, strengthens muscles and bones, \"\n        \"helps maintain a healthy weight, and can improve mental health by reducing anxiety and depression.\",\n        \"ground_truth\": \"Answers should cover physical and mental health benefits\",\n        \"difficulty\": \"medium\",\n        \"category\": \"health\",\n    },\n    {\n        \"id\": \"qa_task_2\",\n        \"question\": \"Describe the main causes of climate change.\",\n        \"reference_output\": \"Climate change is primarily caused by increased concentrations of greenhouse gases \"\n        \"in the atmosphere due to human activities like burning fossil fuels, deforestation, and industrial processes.\",\n        \"ground_truth\": \"Answers should mention greenhouse gases and human activities\",\n        \"difficulty\": \"hard\",\n        \"category\": \"environment\",\n    },\n    {\n        \"id\": \"qa_task_3\",\n        \"question\": \"What is the significance of the Turing Test in AI?\",\n        \"reference_output\": \"The Turing Test, proposed by Alan Turing, is a measure of a machine's ability to exhibit\"\n        \" intelligent behavior equivalent to, or indistinguishable from, that of a human.\",\n        \"ground_truth\": \"Should mention Alan Turing, purpose of the test, and its implications for AI\",\n        \"difficulty\": \"hard\",\n        \"category\": \"technology\",\n    },\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AgentScope Metric vs. OpenJudge Grader\nTo make OpenJudge compatible with AgentScope, we need an adapter that inherits from\nAgentScope's ``MetricBase`` and acts as a bridge to OpenJudge's ``BaseGrader``.\n\n* **AgentScope Metric**: A generic unit of evaluation that accepts a ``SolutionOutput`` and returns a ``MetricResult``.\n* **OpenJudge Grader**: A specialized evaluation unit (e.g., ``RelevanceGrader``) that requires specific, semantic inputs (like ``query``, ``response``, ``context``), and returns a ``GraderResult``.\n\nThis \"Adapter\" allows you to plug *any* OpenJudge grader into your AgentScope benchmark seamlessly.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from openjudge.graders.base_grader import BaseGrader\nfrom openjudge.graders.schema import GraderScore, GraderError\nfrom openjudge.utils.mapping import parse_data_with_mapper\nfrom agentscope.evaluate import (\n    MetricBase,\n    MetricType,\n    MetricResult,\n    SolutionOutput,\n)\n\n\nclass OpenJudgeMetric(MetricBase):\n    \"\"\"\n    A wrapper that converts an OpenJudge grader into an AgentScope Metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        grader_cls: type[BaseGrader],\n        data: dict,\n        mapper: dict,\n        name: str | None = None,\n        description: str | None = None,\n        **grader_kwargs,\n    ):\n        # Initialize the OpenJudge grader\n        self.grader = grader_cls(**grader_kwargs)\n\n        super().__init__(\n            name=name or self.grader.name,\n            metric_type=MetricType.NUMERICAL,\n            description=description or self.grader.description,\n        )\n\n        self.data = data\n        self.mapper = mapper\n\n    async def __call__(self, solution: SolutionOutput) -> MetricResult:\n        \"\"\"Execute the wrapped OpenJudge grader against the agent solution.\"\"\"\n        if not solution.success:\n            return MetricResult(\n                name=self.name,\n                result=0.0,\n                message=\"Solution failed\",\n            )\n\n        try:\n            # 1. Context Construction\n            # Combine Static Task Context (item) and Dynamic Agent Output (solution)\n            combined_data = {\n                \"data\": self.data,\n                \"solution\": {\n                    \"output\": solution.output,\n                    \"meta\": solution.meta,\n                    \"trajectory\": getattr(solution, \"trajectory\", []),\n                },\n            }\n\n            # 2. Data Mapping\n            # Use the mapper to extract 'query', 'response', 'context' from the combined data\n            grader_inputs = parse_data_with_mapper(\n                combined_data,\n                self.mapper,\n            )\n\n            # 3. Evaluation Execution\n            result = await self.grader.aevaluate(**grader_inputs)\n\n            # 4. Result Formatting\n            if isinstance(result, GraderScore):\n                return MetricResult(\n                    name=self.name,\n                    result=result.score,\n                    # We preserve the detailed reasoning provided by OpenJudge\n                    message=result.reason or \"\",\n                )\n            elif isinstance(result, GraderError):\n                return MetricResult(\n                    name=self.name,\n                    result=0.0,\n                    message=f\"Error: {result.error}\",\n                )\n            else:\n                return MetricResult(\n                    name=self.name,\n                    result=0.0,\n                    message=\"Unknown result type\",\n                )\n\n        except Exception as e:\n            return MetricResult(\n                name=self.name,\n                result=0.0,\n                message=f\"Exception: {str(e)}\",\n            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From OpenJudge's Graders to AgentScope's Benchmark\nOpenJudge provides a rich collection of built-in graders. In this example, we select two\ncommon graders suitable for Question-Answering tasks:\n\n* **RelevanceGrader**: Evaluates whether the agent's response directly addresses the user's query.\n* **CorrectnessGrader**: Verifies the factual accuracy of the response against a provided ground truth.\n\n.. tip::\n   OpenJudge offers 50+ built-in graders covering diverse dimensions like **Hallucination**, **Safety**, **Code Quality**,\n   and **JSON Formatting**. Please refer to the [OpenJudge Documentation](https://agentscope-ai.github.io/OpenJudge/built_in_graders/overview/)\n   for the full list of available graders.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Ensure you have set your ``DASHSCOPE_API_KEY`` environment variable before running the example below.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom typing import Generator\nfrom openjudge.graders.common.relevance import RelevanceGrader\nfrom openjudge.graders.common.correctness import CorrectnessGrader\nfrom agentscope.evaluate import (\n    Task,\n    BenchmarkBase,\n)\n\n\nclass QABenchmark(BenchmarkBase):\n    \"\"\"A benchmark for QA tasks using OpenJudge metrics.\"\"\"\n\n    def __init__(self):\n        super().__init__(\n            name=\"QA Quality Benchmark\",\n            description=\"Benchmark to evaluate QA systems using OpenJudge grader classes\",\n        )\n        self.dataset = self._load_data()\n\n    def _load_data(self):\n        tasks = []\n        # Configuration for LLM-based graders\n        # Ensure OPENAI_API_KEY is set in your environment variables\n        model_config = {\n            \"model\": \"qwen3-32b\",\n            \"api_key\": os.environ.get(\"DASHSCOPE_API_KEY\"),\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        }\n\n        for data in QA_BENCHMARK_DATASET:\n            # Define the Mapping: Left is OpenJudge key, Right is AgentScope path\n            mapper = {\n                \"query\": \"data.input\",\n                \"response\": \"solution.output\",\n                \"context\": \"data.ground_truth\",\n                \"reference_response\": \"data.reference_output\",\n            }\n\n            # Instantiate Metrics via Wrapper\n            metrics = [\n                OpenJudgeMetric(\n                    grader_cls=RelevanceGrader,\n                    data=data,\n                    mapper=mapper,\n                    name=\"Relevance\",\n                    model=model_config,\n                ),\n                OpenJudgeMetric(\n                    grader_cls=CorrectnessGrader,\n                    data=data,\n                    mapper=mapper,\n                    name=\"Correctness\",\n                    model=model_config,\n                ),\n            ]\n\n            # Create Task\n            task = Task(\n                id=data[\"id\"],\n                input=data[\"question\"],\n                ground_truth=data[\"ground_truth\"],\n                metrics=metrics,\n            )\n\n            tasks.append(task)\n\n        return tasks\n\n    def __iter__(self) -> Generator[Task, None, None]:\n        \"\"\"Iterate over the benchmark.\"\"\"\n        yield from self.dataset\n\n    def __getitem__(self, index: int) -> Task:\n        \"\"\"Get a task by index.\"\"\"\n        return self.dataset[index]\n\n    def __len__(self) -> int:\n        \"\"\"Get the length of the benchmark.\"\"\"\n        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Evaluation\nFinally, use AgentScope's ``GeneralEvaluator`` to run the benchmark on a QA agent.\nThe results will include both the **Quantitative Score** and the **Qualitative Reasoning**\nfrom the OpenJudge graders.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n\nfrom agentscope.agent import ReActAgent\nfrom agentscope.evaluate import GeneralEvaluator\nfrom agentscope.evaluate import FileEvaluatorStorage\nfrom agentscope.formatter import DashScopeChatFormatter\nfrom agentscope.message import Msg\nfrom agentscope.model import OpenAIChatModel\n\n\nasync def qa_agent(task: Task, pre_hook: Callable) -> SolutionOutput:\n    \"\"\"Solution function that generates answers to QA tasks.\"\"\"\n\n    model = OpenAIChatModel(\n        model_name=\"qwen3-32b\",\n        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    )\n\n    # Create a QA agent\n    agent = ReActAgent(\n        name=\"QAAgent\",\n        sys_prompt=\"You are an expert at answering questions. Provide clear, accurate, and comprehensive answers.\",\n        model=model,\n        formatter=DashScopeChatFormatter(),\n    )\n\n    # Generate response\n    msg_input = Msg(name=\"User\", content=task.input, role=\"user\")\n    response = await agent(msg_input)\n    response_text = response.content\n\n    return SolutionOutput(\n        success=True,\n        output=response_text,\n        trajectory=[\n            task.input,\n            response_text,\n        ],  # Store the interaction trajectory\n    )\n\n\nasync def main() -> None:\n    evaluator = GeneralEvaluator(\n        name=\"OpenJudge Integration Demo\",\n        benchmark=QABenchmark(),\n        # Repeat how many times\n        n_repeat=1,\n        storage=FileEvaluatorStorage(\n            save_dir=\"./results\",\n        ),\n        # How many workers to use\n        n_workers=1,\n    )\n\n    await evaluator.run(qa_agent)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}