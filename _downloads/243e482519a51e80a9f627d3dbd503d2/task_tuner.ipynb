{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Tuner\n\nAgentScope provides the ``tuner`` module for training agent applications using reinforcement learning (RL).\nThis tutorial will guide you through how to leverage the ``tuner`` module to improve agent performance on specific tasks, including:\n\n- Introducing the core components of the ``tuner`` module\n- Demonstrating the key code required for the tuning workflow\n- Showing how to configure and run the tuning process\n\n## Main Components\nThe ``tuner`` module introduces three core components essential for RL-based agent training:\n\n- **Task Dataset**: A collection of tasks for training and evaluating the agent.\n- **Workflow Function**: Encapsulates the agent's logic to be tuned.\n- **Judge Function**: Evaluates the agent's performance on tasks and provides reward signals for tuning.\n\nIn addition, ``tuner`` provides several configuration classes for customizing the tuning process, including:\n\n- **TunerModelConfig**: Model configurations for tuning purposes.\n- **AlgorithmConfig**: Specifies the RL algorithm (e.g., GRPO, PPO) and its parameters.\n\n## Implementation\nThis section demonstrates how to use ``tuner`` to train a simple math agent.\n\n### Task Dataset\nThe task dataset contains tasks for training and evaluating your agent.\n\nYou dataset should follow the Huggingface [datasets](https://huggingface.co/docs/datasets/quickstart) format, which can be loaded with ``datasets.load_dataset``. For example:\n\n```text\nmy_dataset/\n    \u251c\u2500\u2500 train.jsonl  # training samples\n    \u2514\u2500\u2500 test.jsonl   # evaluation samples\n```\nSuppose your `train.jsonl` contains:\n\n```json\n{\"question\": \"What is 2 + 2?\", \"answer\": \"4\"}\n{\"question\": \"What is 4 + 4?\", \"answer\": \"8\"}\n```\nBefore starting tuning, you can verify that your dataset is loaded correctly with:\n\n```python\nfrom agentscope.tuner import DatasetConfig\n\ndataset = DatasetConfig(path=\"my_dataset\", split=\"train\")\ndataset.preview(n=2)\n# Output the first two samples to verify correct loading\n# [\n#   {\n#     \"question\": \"What is 2 + 2?\",\n#     \"answer\": \"4\"\n#   },\n#   {\n#     \"question\": \"What is 4 + 4?\",\n#     \"answer\": \"8\"\n#   }\n# ]\n```\n### Workflow Function\nThe workflow function defines how the agent interacts with the environment and makes decisions. All workflow functions should follow the input/output signature defined in ``agentscope.tuner.WorkflowType``.\n\nBelow is an example workflow function using a ReAct agent to answer math questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Optional\nfrom agentscope.agent import ReActAgent\nfrom agentscope.formatter import OpenAIChatFormatter\nfrom agentscope.message import Msg\nfrom agentscope.model import ChatModelBase\nfrom agentscope.tuner import WorkflowOutput\n\n\nasync def example_workflow_function(\n    task: Dict,\n    model: ChatModelBase,\n    auxiliary_models: Optional[Dict[str, ChatModelBase]] = None,\n) -> WorkflowOutput:\n    \"\"\"An example workflow function for tuning.\n\n    Args:\n        task (`Dict`): The task information.\n        model (`ChatModelBase`): The chat model used by the agent.\n        auxiliary_models (`Optional[Dict[str, ChatModelBase]]`): Additional\n            chat models, generally used to simulate the behavior of other\n            non-training agents in multi-agent scenarios.\n\n    Returns:\n        `WorkflowOutput`: The output generated by the workflow.\n    \"\"\"\n    agent = ReActAgent(\n        name=\"react_agent\",\n        sys_prompt=\"You are a helpful math problem solving agent.\",\n        model=model,\n        formatter=OpenAIChatFormatter(),\n    )\n\n    response = await agent.reply(\n        msg=Msg(\n            \"user\",\n            task[\"question\"],\n            role=\"user\",\n        ),  # extract question from task\n    )\n\n    return WorkflowOutput(  # return the response\n        response=response,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can directly run this workflow function with a task dictionary and a ``DashScopeChatModel`` / ``OpenAIChatModel`` to test its correctness before formal training. For example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nimport os\nfrom agentscope.model import DashScopeChatModel\n\ntask = {\"question\": \"What is 123 plus 456?\", \"answer\": \"579\"}\nmodel = DashScopeChatModel(\n    model_name=\"qwen-max\",\n    api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n)\nworkflow_output = asyncio.run(example_workflow_function(task, model))\nassert isinstance(\n    workflow_output.response,\n    Msg,\n), \"In this example, the response should be a Msg instance.\"\nprint(\"\\nWorkflow response:\", workflow_output.response.get_text_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Judge Function\nThe judge function evaluates the agent's performance on a given task and provides a reward signal for tuning.\nAll judge functions should follow the input/output signature defined in ``agentscope.tuner.JudgeType``.\nBelow is a simple judge function that compares the agent's response with the ground truth answer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Any\nfrom agentscope.tuner import JudgeOutput\n\n\nasync def example_judge_function(\n    task: Dict,\n    response: Any,\n    auxiliary_models: Optional[Dict[str, ChatModelBase]] = None,\n) -> JudgeOutput:\n    \"\"\"A very simple judge function only for demonstration.\n\n    Args:\n        task (`Dict`): The task information.\n        response (`Any`): The response field from the WorkflowOutput.\n        auxiliary_models (`Optional[Dict[str, ChatModelBase]]`): Additional\n            chat models for LLM-as-a-Judge purpose.\n    Returns:\n        `JudgeOutput`: The reward assigned by the judge.\n    \"\"\"\n    ground_truth = task[\"answer\"]\n    reward = 1.0 if ground_truth in response.get_text_content() else 0.0\n    return JudgeOutput(reward=reward)\n\n\njudge_output = asyncio.run(\n    example_judge_function(\n        task,\n        workflow_output.response,\n    ),\n)\nprint(f\"Judge reward: {judge_output.reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The judge function can also be locally tested in the same way as shown above before formal training to ensure its logic is correct.\n\n.. tip::\n   You can leverage existing [MetricBase](https://github.com/agentscope-ai/agentscope/blob/main/src/agentscope/evaluate/_metric_base.py) implementations in your judge function to compute more sophisticated metrics and combine them into a composite reward.\n\n## Configuration and Running\nFinally, you can configure and run the tuning process using the ``tuner`` module.\nBefore starting, ensure that [Trinity-RFT](https://github.com/agentscope-ai/Trinity-RFT) is installed in your environment, as it is required for tuning.\n\nBelow is an example of configuring and starting the tuning process:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example is for demonstration only. For a complete runnable example, see [Tune ReActAgent](https://github.com/agentscope-ai/agentscope/tree/main/examples/tuner/react_agent)</p></div>\n\n```python\nfrom agentscope.tuner import tune, AlgorithmConfig, DatasetConfig, TunerModelConfig\n# your workflow / judge function here...\n\nif __name__ == \"__main__\":\n    dataset = DatasetConfig(path=\"my_dataset\", split=\"train\")\n    model = TunerModelConfig(model_path=\"Qwen/Qwen3-0.6B\", max_model_len=16384)\n    algorithm = AlgorithmConfig(\n        algorithm_type=\"multi_step_grpo\",\n        group_size=8,\n        batch_size=32,\n        learning_rate=1e-6,\n    )\n    tune(\n        workflow_func=example_workflow_function,\n        judge_func=example_judge_function,\n        model=model,\n        train_dataset=dataset,\n        algorithm=algorithm,\n    )\n```\nHere, ``DatasetConfig`` configures the training dataset, ``TunerModelConfig`` sets the parameters for the trainable model, and ``AlgorithmConfig`` specifies the reinforcement learning algorithm and its hyperparameters.\n\n.. tip::\n   The ``tune`` function is based on [Trinity-RFT](https://github.com/agentscope-ai/Trinity-RFT) and internally converts input parameters to a YAML configuration.\n   Advanced users can skip the ``model``, ``train_dataset``, and ``algorithm`` arguments and instead provide a YAML config file path via the ``config_path`` argument.\n   Using a configuration file is recommended for fine-grained control and to leverage advanced Trinity-RFT features. See the Trinity-RFT [Configuration Guide](https://agentscope-ai.github.io/Trinity-RFT/en/main/tutorial/trinity_configs.html) for more options.\n\nSave the above code as ``main.py`` and run it with:\n\n```bash\nray start --head\npython main.py\n```\nCheckpoints and logs are automatically saved to the ``checkpoints/AgentScope`` directory under your workspace, with each run in a timestamped sub-directory. Tensorboard logs can be found in ``monitor/tensorboard`` within the checkpoint directory.\n\n```text\nyour_workspace/\n    \u2514\u2500\u2500 checkpoints/\n        \u2514\u2500\u2500AgentScope/\n            \u2514\u2500\u2500 Experiment-20260104185355/  # each run saved in a sub-directory with timestamp\n                \u251c\u2500\u2500 monitor/\n                \u2502   \u2514\u2500\u2500 tensorboard/  # tensorboard logs\n                \u2514\u2500\u2500 global_step_x/    # saved model checkpoints at step x\n```\n.. tip::\n   For more tuning examples, refer to the [tuner directory](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner) of the AgentScope-Samples repository.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}